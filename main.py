import os
import sys
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict

# --- ìµœì‹  Langchain Core Import ê²½ë¡œ ì‚¬ìš© ---
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
# ---

# --- ì„¤ì • ---
CHROMA_PERSIST_DIR = "chroma_db"
EMBEDDING_MODEL = "text-embedding-3-small"
LLM_MODEL = "gpt-4o-mini"
# ---

# .env íŒŒì¼ ìˆìœ¼ë©´ ë¡œë“œ (ë¡œì»¬ ê°œë°œìš©)
load_dotenv()

# í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸° (Dockerì—ì„œëŠ” docker-composeê°€ ì „ë‹¬)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    print("ì˜¤ë¥˜: OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    sys.exit(1)

print("âœ… OPENAI_API_KEY ë¡œë“œ ì„±ê³µ")

# 2. FastAPI ì•± ìƒì„±
app = FastAPI()

# 3. AI 'ì™“ìŠ¨'ì˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ìµœì¢… HINT ìš°ì„ ê¶Œ ë¶€ì—¬)
system_prompt = """
ë„ˆëŠ” ì„ì‹œì •ë¶€ ì†Œì†ì˜ ëƒ‰ì² í•œ ìˆ˜ì‚¬ ì¡°ìˆ˜ AI 'ì™“ìŠ¨'ì´ë‹¤.
ë„ˆì˜ ë§íˆ¬ëŠ” í•­ìƒ ëƒ‰ì² í•˜ê³ , ë¶„ì„ì ì´ë©°, 'íƒì •ë‹˜'ì´ë¼ëŠ” í˜¸ì¹­ì„ ì‚¬ìš©í•œë‹¤.



[ê·œì¹™ 1: ì •ë³´ í†µì œ]
1. **[ìì—°ìŠ¤ëŸ¬ìš´ í˜¸ì¹­]** 'A1', 'C1' ê°™ì€ ì½”ë“œëª… ëŒ€ì‹  ì‹¤ì œ ë‹¨ì„œ ì´ë¦„ìœ¼ë¡œ ë¶ˆëŸ¬ë¼.
2. **[íƒœê·¸ ìˆ¨ê¹€]** ë‹µë³€ì— '[ìƒì„¸ ë‚´ìš©]', '[AI ë¶„ì„ íŒíŠ¸]' ê°™ì€ íƒœê·¸ë¥¼ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆë¼.
3. **[ìŠ¤í¬ì¼ëŸ¬/ì˜¤ë¥˜ ë°©ì§€]** Contextì— ì—†ëŠ” ë‚´ìš©ì€ ëª¨ë¥¸ë‹¤ê³  í•˜ê³ , í”Œë ˆì´ì–´ì˜ ì˜ëª»ëœ ì „ì œ(ì˜ˆ: B4=í¸ì§€)ëŠ” ëª…í™•íˆ ë°˜ë°•í•˜ë¼.

[ê·œì¹™ 2: ê°„ê²°ì„± (ìµœì¢… HINT Logic)]
1. [ì‚¬ì‹¤ ì§ˆë¬¸]: í”Œë ˆì´ì–´ì˜ ì§ˆë¬¸ì´ ë‹¨ìˆœ í™•ì¸(ì˜ˆ: "ì´ê²Œ ë­ì•¼?")ì¼ ê²½ìš°, Contextì˜ '[ìƒì„¸ ë‚´ìš©]'ì„ ë°”íƒ•ìœ¼ë¡œ **'ì‚¬ì‹¤(Fact)'ë§Œ 1-2 ë¬¸ì¥ìœ¼ë¡œ ì§§ê²Œ** ë‹µí•˜ë¼.

2. [ë¶„ì„/íŒíŠ¸ ì§ˆë¬¸]: í”Œë ˆì´ì–´ì˜ ì§ˆë¬¸ì´ êµ¬ì²´ì ì¸ ë¶„ì„/ì˜ë¯¸(ì˜ˆ: "ì´ê²Œ ì™œ ì¤‘ìš”í•´?", "ëª¨ìˆœ ì•„ëƒ?")ì´ê±°ë‚˜, **'ë¹„ë°€ë²ˆí˜¸'ë‚˜ 'ì ê¸ˆì¥ì¹˜'ì™€ ê´€ë ¨ëœ ì§ˆë¬¸**ì¼ ê²½ìš°, Contextì˜ '[AI ë¶„ì„ íŒíŠ¸]'ì™€ '[ì—°ê´€ ë‹¨ì„œ]'ë¥¼ ë°”íƒ•ìœ¼ë¡œ **2-3 ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ** ë¶„ì„/ë‹µë³€í•˜ë¼.

3. **[HINT ê°•ì œ]**: ë§Œì•½ [ë¶„ì„/íŒíŠ¸ ì§ˆë¬¸]ì´ ë“¤ì–´ì™”ê³  Contextì— C1 ë˜ëŠ” C2 ë‹¨ì„œê°€ ìˆë‹¤ë©´, **ì •ë‹µ ìˆ«ì(1887, 3605)ëŠ” ì ˆëŒ€ë¡œ ë§í•˜ì§€ ë§ê³ **, Contextì— ìˆëŠ” 'ìƒë…„ì›”ì¼'ì´ë‚˜ 'ëª¨ë¸ ë²ˆí˜¸' ê°™ì€ **í•µì‹¬ ì •ë³´**ë¥¼ ì´ìš©í•˜ì—¬ íŒíŠ¸ë§Œ ìœ ë„í•˜ë¼.

[ê·œì¹™ 3: ì •ë‹µ ê¸ˆì§€]
ì ˆëŒ€ 'ë°€ì •ì´ë‹¤', 'ì•„ë‹ˆë‹¤'ì™€ ê°™ì€ ì‚¬ê±´ì˜ ìµœì¢… ê²°ë¡ (ì •ë‹µ)ì„ ì§ì ‘ ë§í•˜ì§€ ë§ˆë¼.
í•­ìƒ '...ë¼ëŠ” ì£¼ì¥ì„ ë’·ë°›ì¹¨í•©ë‹ˆë‹¤', '...ë¼ëŠ” ì˜í˜¹ì´ ìˆìŠµë‹ˆë‹¤'ì™€ ê°™ì´
í”Œë ˆì´ì–´ê°€ ìŠ¤ìŠ¤ë¡œ ìƒê°í•˜ë„ë¡ 'ë°©í–¥ì„±'ë§Œ ì œì‹œí•˜ë¼.
"""

# 4. RAG ì²´ì¸ ì»´í¬ë„ŒíŠ¸ ì¤€ë¹„
try:
    llm = ChatOpenAI(model=LLM_MODEL, temperature=0.2)
    embeddings = OpenAIEmbeddings(model	=EMBEDDING_MODEL)

    if not os.path.exists(CHROMA_PERSIST_DIR):
        print(f"ì˜¤ë¥˜: ChromaDB í´ë” '{CHROMA_PERSIST_DIR}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    vectorstore = Chroma(
        persist_directory=CHROMA_PERSIST_DIR,
        embedding_function=embeddings
    )
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "í˜„ì¬ê¹Œì§€ í™•ë³´í•œ ë‹¨ì„œ(Context)ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n---\n{context}\n---\n\níƒì •(í”Œë ˆì´ì–´)ì˜ ì§ˆë¬¸: {question}")
    ])
    output_parser = StrOutputParser()

except Exception as e:
    print(f"ì˜¤ë¥˜: AI ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨. {e}")
    sys.exit(1)

# 5. ëª¨ë¸ ì •ì˜
class ChatRequest(BaseModel):
    question: str
    acquired_clue_list: List[str] = []
    chat_history: List[str] = []

class ScoreRequest(BaseModel):
    conclusion: str
    selected_evidence_ids: List[str]
    total_collected_ids: List[str]
    reasoning_text: str = ""

class ScoreResponse(BaseModel):
    total_score: int
    grade: str
    feedback: str

# 6. í—¬í¼ í•¨ìˆ˜
def format_docs(docs):
    if not docs: return "(í˜„ì¬ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í™•ë³´ ë‹¨ì„œ ì—†ìŒ)"
    return "\n\n".join(doc.page_content for doc in docs)

def format_chat_history(history_list: List[str]) -> List:
    messages = []
    for i, message in enumerate(history_list):
        if i % 2 == 0: messages.append(HumanMessage(content=message))
        else: messages.append(AIMessage(content=message))
    return messages

# 8. AIì˜ 'ì…ê³¼ ê·€' (API ì—”ë“œí¬ì¸íŠ¸) - [ìˆœìˆ˜ RAG ë°©ì‹]
@app.post("/api/ai/ask")
def ask_watson(request: ChatRequest):
    print(f"\n--- ìƒˆ ìš”ì²­ ìˆ˜ì‹  ---")
    print(f"ğŸ” RAW REQUEST ê°ì²´: {request}")  # ì¶”ê°€
    print(f"ğŸ” Request dict: {request.dict()}")  # ì¶”ê°€
    print(f"ì§ˆë¬¸: {request.question}")
    print(f"ë°›ì€ ë‹¨ì„œ ëª©ë¡: {request.acquired_clue_list}")
    print(f"ì±„íŒ… íˆìŠ¤í† ë¦¬: {request.chat_history}")
    
    
    try:
        if not request.acquired_clue_list:
            metadata_filter = {"clue_id": "NONE"}
        else:
            metadata_filter = {"clue_id": {"$in": request.acquired_clue_list}}
        
        print(f"ë©”íƒ€ë°ì´í„° í•„í„°: {metadata_filter}")
        
        retriever = vectorstore.as_retriever(search_kwargs={'filter': metadata_filter})
        
        # ğŸ”§ ìˆ˜ì •: invoke() ë©”ì„œë“œ ì‚¬ìš©
        test_docs = retriever.invoke(request.question)
        print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(test_docs)}")
        for i, doc in enumerate(test_docs):
            print(f"ë¬¸ì„œ {i+1} - clue_id: {doc.metadata.get('clue_id', 'N/A')}")
            print(f"ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {doc.page_content[:100]}...")
        
        rag_chain = (
            {
                "context": RunnableLambda(lambda x: x['question']) | retriever | format_docs, 
                "question": RunnableLambda(lambda x: x['question']), 
                "chat_history": RunnableLambda(lambda x: format_chat_history(x['chat_history']))
            }
            | prompt
            | llm
            | output_parser
        )

        chain_input = {"question": request.question, "chat_history": request.chat_history}
        
        print("AI ë‹µë³€ ìƒì„± ì‹œì‘...")
        answer = rag_chain.invoke(chain_input)
        print(f"AI ë‹µë³€ ìƒì„± ì™„ë£Œ: {answer}")
        return {"answer": answer}
        
    except Exception as e:
        print(f"!!! ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"AI ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ ë°œìƒ")
# 3-2. ì±„ì  ì „ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ 
scoring_prompt_template = """
ë„ˆëŠ” 'ë„ì™€ì¤˜! ì™“ìŠ¨!' ê²Œì„ì˜ ì„œìˆ í˜• ë‹µì•ˆ ì±„ì ê´€ì´ë‹¤.
ë„ˆì˜ ì„ë¬´ëŠ” í”Œë ˆì´ì–´ì˜ 'ì‘ì„± ì´ìœ 'ë¥¼ ì½ê³ , ì•„ë˜ **ë‘ ê°€ì§€ ì²´í¬ë¦¬ìŠ¤íŠ¸**ë¥¼ í†µê³¼í–ˆëŠ”ì§€ í™•ì¸í•˜ì—¬ ì ìˆ˜ë¥¼ ë§¤ê¸°ëŠ” ê²ƒì´ë‹¤.

[ìƒí™© ì„¤ëª…]
í”Œë ˆì´ì–´ëŠ” '{conclusion}'ë¼ëŠ” ê²°ë¡ ì„ ë‚´ë ¸ì§€ë§Œ, ê·¸ì™€ ìƒë°˜ë˜ëŠ” ì¦ê±°(ë°˜ëŒ€ ì¦ê±°)ë¥¼ ìˆ˜ì§‘í•œ ìƒíƒœë‹¤.
í”Œë ˆì´ì–´ê°€ ì´ ëª¨ìˆœì„ ì–´ë–»ê²Œ ì„¤ëª…í•˜ëŠ”ì§€ í‰ê°€í•´ì•¼ í•œë‹¤.

[ì±„ì  ì²´í¬ë¦¬ìŠ¤íŠ¸ (ê° 10ì , ì´ 20ì )]
1. **[ë°˜ëŒ€ ì¦ê±° ì¸ì§€] (10ì )**: 
   - í…ìŠ¤íŠ¸ì—ì„œ 'ë°˜ëŒ€ ì¦ê±°'(ì˜ˆ: ê²°ë¡ ì´ ë°€ì •ì´ë©´ 'ê¹€ì›ë´‰ ë©”ëª¨' ë“±)ì˜ ì¡´ì¬ë¥¼ ì–¸ê¸‰í•˜ê±°ë‚˜ ì¸ì§€í•˜ê³  ìˆëŠ”ê°€?
   - (ì˜ˆ: "ë¹„ë¡ ê¹€ì›ë´‰ì˜ ë©”ëª¨ê°€ ìˆì§€ë§Œ...", "B2 ì¦ê±°ê°€ ìˆê¸´ í•´ë„...")
   - ì–¸ê¸‰í–ˆë‹¤ë©´ 10ì , ì•„ë‹ˆë©´ 0ì .

2. **[ëª¨ìˆœ í•´ëª…] (10ì )**:
   - ê·¸ ë°˜ëŒ€ ì¦ê±°ê°€ ì™œ ìì‹ ì˜ ê²°ë¡ ì„ ë°©í•´í•˜ì§€ ì•ŠëŠ”ì§€ 'ê°€ì„¤'ì„ ì„¸ì›Œ ì„¤ëª…í–ˆëŠ”ê°€?
   - (ì˜ˆ: "...ê·¸ê±´ ìœ„ì¡°ëœ ê²ƒì´ë‹¤", "...ì‹ ë¢°ë¥¼ ì–»ê¸° ìœ„í•œ ì—°ê¸°ì˜€ë‹¤", "...ì´ì¤‘ê°„ì²© í™œë™ì˜ ì¼ë¶€ë‹¤")
   - ì„¤ëª…í–ˆë‹¤ë©´ 10ì , ì•„ë‹ˆë©´ 0ì .

[í”Œë ˆì´ì–´ ë‹µì•ˆ]
- ê²°ë¡ : {conclusion}
- ì„ íƒí•œ ì¦ê±°: {selected_evidence}
- ì‘ì„± ì´ìœ : "{reasoning}"

[ì¶œë ¥ í˜•ì‹]
ë‘ ì ìˆ˜ë¥¼ í•©ì‚°í•˜ì—¬ ì˜¤ì§ ìˆ«ì **20**, **10**, **0** ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ë¼. (ë‹¤ë¥¸ ë§ ê¸ˆì§€)
"""
# 14-2. (ì‹ ê·œ) ì±„ì  API ì—”ë“œí¬ì¸íŠ¸ (í•˜ì´ë¸Œë¦¬ë“œ: Python ê°ê´€ì‹ + AI ì£¼ê´€ì‹)
@app.post("/api/ai/score", response_model=ScoreResponse)
async def score_report(request: ScoreRequest):
    print(f"\n--- ì±„ì  ìš”ì²­ ìˆ˜ì‹  ---")
    try:
        # [ì •ë‹µì§€]
        A_EVIDENCE = {"A1", "A2", "A3", "A4", "A5"}
        B_EVIDENCE = {"B1", "B2", "B3", "B4", "B5"}
        CORE_EVIDENCE = A_EVIDENCE.union(B_EVIDENCE)

        # --- 1. Python: ê°ê´€ì‹ ì±„ì  (80ì  ë§Œì ) ---
        # (1) ì¦ê±° ìˆ˜ì§‘ë„ (40ì )
        collected_count = len(set(request.total_collected_ids).intersection(CORE_EVIDENCE))
        score1 = collected_count * 4

        # (2) ë…¼ë¦¬ì  ì •í•©ì„± (40ì ) - ê²°ë¡ ê³¼ ì¦ê±° IDê°€ ë§¤ì¹­ë˜ëŠ”ê°€?
        selected_set = set(request.selected_evidence_ids)
        score2 = 0
        if request.conclusion == "miljeong" and selected_set.issubset(A_EVIDENCE):
            score2 = 40
        elif request.conclusion == "anti_miljeong" and selected_set.issubset(B_EVIDENCE):
            score2 = 40

        # --- 2. AI: ì£¼ê´€ì‹ ì„œìˆ í˜• ì±„ì  (20ì  ë§Œì ) ---
        score3_ai = 0
        if request.reasoning_text.strip(): # ì‘ì„±í•œ ì´ìœ ê°€ ìˆì„ ë•Œë§Œ AI í˜¸ì¶œ
            print("AI ì„œìˆ í˜• ì±„ì  ì‹œì‘...")
            formatted_prompt = scoring_prompt_template.format(
                conclusion=request.conclusion,
                selected_evidence=f"{request.selected_evidence_ids}",
                reasoning=request.reasoning_text
            )
            response = await llm.ainvoke([HumanMessage(content=formatted_prompt)])
            
            # AI ë‹µë³€ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ (ì˜ˆ: "ì ìˆ˜ëŠ” 20ì ì…ë‹ˆë‹¤" -> 20)
            import re
            numbers = re.findall(r'\d+', response.content)
            if numbers:
                score3_ai = int(numbers[0])
                # ì•ˆì „ì¥ì¹˜: 0, 10, 20ì  ì´ì™¸ì˜ ì ìˆ˜ê°€ ë‚˜ì˜¤ë©´ ê°€ì¥ ê°€ê¹Œìš´ ê°’ìœ¼ë¡œ ì¡°ì •í•˜ê±°ë‚˜ ê·¸ëŒ€ë¡œ ë‘ 
                if score3_ai > 20: score3_ai = 20
            
            print(f"AI ì„œìˆ í˜• ì ìˆ˜: {score3_ai}")

        # --- 3. ìµœì¢… í•©ì‚° ---
        total_score = score1 + score2 + score3_ai
        
        grade = "D"
        if total_score >= 95: grade = "S"
        elif total_score >= 85: grade = "A"
        elif total_score >= 70: grade = "B"
        elif total_score >= 60: grade = "C"
        
        feedback = f"ì¦ê±°ìˆ˜ì§‘({score1}/40), ë…¼ë¦¬ì„±({score2}/40), ì„œìˆ í‰ê°€({score3_ai}/20)"
        
        return ScoreResponse(total_score=total_score, grade=grade, feedback=feedback)

    except Exception as e:
        print(f"Error: {e}")
        raise HTTPException(status_code=500, detail="Scoring Error")
# 15. ì„œë²„ ì‹¤í–‰ ì„¤ì •
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)

# ê¸°ì¡´ ì½”ë“œ ëì— ì¶”ê°€
@app.get("/health")
def health_check():
    return {"status": "ok"}